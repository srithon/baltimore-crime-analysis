#+title: Baltimore Crime Analysis
#+property: header-args:python :session ./.jupyter_confile.json :kernel python3 :results output :noweb yes
#+property: CLEAN-EXPORT-FILENAME ./baltimore-crime-analysis_clean.ipynb.org

* TODO Introduction
** TODO Description of Project :ignore:
* General Setup
Let's import the core libraries that we will be using in our project.
#+begin_src python
import requests
import pandas as pd
import plotly.express as px
import geopandas as gpd

from bs4 import BeautifulSoup
import re
from time import sleep
#+end_src

* TODO Data Collection
We want all of our data to lie in the ~data/~ subdirectory.

First, let's make a SPEC matching each dataset/topic to file(s) that they will be saved in.
#+begin_src python
data_files = {
    'crime-data': 'crime_data.csv',
    'crime-codes': 'crime_codes.csv',

    'police-stations': 'police_stations.csv',
    'population-density': 'total_populations.csv',
    'median-income': 'median_incomes.geojson',
}
#+end_src

We only want to download the data once, so that future re-runs do not result in downloading it again.
To accomplish this, we will make a distinction between *static* and *dynamic* data.
* Static* data is data that we can directly from the internet without modification.
* Dynamic* data is data that we /compute/ from static data.
The benefit of this is that when we change our dynamic computation logic, we do not have to redownload our static data.

This function will, given the "tag" of the topic, yield a filepath for the static data.
For example, ~static_filepath('crime-data')~ would yield ~data/static/crime_data.csv~.
#+begin_src python
def static_filepath(tag) -> str:
    return f'data/static/{data_files[tag]}'
#+end_src

Similarly, ~dynamic_filepath~ will yield the filepath after post-processing.
#+begin_src python
def dynamic_filepath(tag) -> str:
    return f'data/dynamic/{data_files[tag]}'
#+end_src

Now, let's make our static and dynamic directories incase they don't exist.
#+begin_src python
import os
for d in ['dynamic', 'static']:
    try:
        os.mkdir(f'data/{d}')
    except FileExistsError:
        pass
#+end_src

** Helper Functions

*** Rate-limiting
Let's write a higher-order function that, given an Iterable, will return a Generator which will wait a certain amount of time between yielding elements.
#+begin_src python
from typing import Iterable, Generator

def delay_iterable(iterable: Iterable, delay_seconds=0.5) -> Generator:
    is_first_element = True
    for element in iterable:
        if not is_first_element:
            sleep(delay_seconds)
        else:
            is_first_element = False

        yield element
#+end_src

Let's write another function for simply delaying repeated function calls.
#+begin_src python
def delay_fn(f, delay_seconds=0.5):
    is_first_time = True

    def delayed_fn(*args, **kwargs):
        nonlocal f, is_first_time

        if not is_first_time:
            sleep(delay_seconds)
        else:
            is_first_time = False

        return f(*args, **kwargs)

    return delayed_fn
#+end_src
*** Caching Data
We don't want to refetch our data every time we run the notebook, so let's write some helper functions that allow us to cache our data after we download it.
#+begin_src python
from os import path

def download_cached(url: str, filepath: str):
    # if the file already exists, does nothing.
    # otherwise, downloads to filepath
    if not path.exists(filepath):
        with open(filepath, 'w+') as f:
            f.write(requests.get(url).text)
#+end_src
*** Normalizing Data
This function will normalize a Series between -1 and 1.
This makes things easier for our machine learning model.
#+begin_src python
def normalize_min_max(series: 'Series') -> 'Series':
    return 2 * (series - series.min()) / (series.max() - series.min()) - 1
#+end_src

** One-time Data Collection
*** Crime Data
In this section, we are going to use the [[https://data.world/baltimore/baltimore-crime-data][Baltimore Crime Data]] dataset, published by the city of Baltimore itself.
Let's start off by downloading the data.
#+begin_src python
download_url = r'https://download.data.world/file_download/baltimore/baltimore-crime-data/BPD_Part_1_Victim_Based_Crime_Data.csv'

download_filepath = static_filepath('crime-data')
download_cached(download_url, download_filepath)
#+end_src

By using ~download_cached~, we will only download the file if it does not already exist.
Now, let's look at the data and see how we can clean it up.
#+begin_src python
crime_df = pd.read_csv(download_filepath)
print(crime_df.head())
#+end_src

First, let's convert ~CrimeDate~ and ~CrimeTime~, both currently strings, into a single ~CrimeDateTime~ column.
Looking at the data, we have certain ~CrimeTime~ entries which seem to have the format ~<hour><minute>~ instead of the regular ~<hour>:<minute>:<seconds>~ format.
Before we convert, we have to first normalize the outliers.
#+begin_src python
bad_time_format = re.compile('^(\d{2})(\d{2})$')
crime_df['CrimeTime'] = crime_df['CrimeTime'].str.replace(bad_time_format, r'\1:\2:00')
#+end_src

Let's make sure that all of the rows match our expected regex by printing out all of the times that don't match what we're expecting.
#+begin_src python
expected_time_format = re.compile('^\d{2}:\d{2}:\d{2}$')
print(crime_df[~crime_df['CrimeTime'].str.fullmatch(expected_time_format)]['CrimeTime'])
#+end_src

The list is empty, so we're good.
Now, let's do the ~DateTime~ conversion.
#+begin_src python
crime_time = pd.to_timedelta(crime_df['CrimeTime'])
crime_df['CrimeDateTime'] = pd.to_datetime(crime_df['CrimeDate'], format='%m/%d/%Y') + crime_time
print(crime_df.head())
#+end_src

Now, let's drop the initial Date and Time columns.
#+begin_src python
crime_df = crime_df.drop(['CrimeDate', 'CrimeTime'], axis=1)
#+end_src

~CrimeCode~ seems important, but in its current form it's not very useful.
To know what each crime code means, let's download the companion [[https://data.baltimorecity.gov/documents/e6ca4eadecdc475a961f68bc314e2a86/about][CRIME CODES]] dataset.
#+begin_src python
download_url = r'https://www.arcgis.com/sharing/rest/content/items/e6ca4eadecdc475a961f68bc314e2a86/data'
download_filepath = static_filepath('crime-codes')

download_cached(download_url, download_filepath)
#+end_src

Let's see what the data looks like before moving on.
#+begin_src python
crime_codes_df = pd.read_csv(download_filepath)
print(crime_codes_df.head())
#+end_src

*** Police Stations
There are 9 districts in Baltimore, corresponding to the 4 cardinal directions, 4 in-betweens and one central district.
To get the locations of the police stations in Baltimore, we will webscrape https://www.baltimorepolice.org/find-my-district, get the addresses of each station, and then use ~geopy~ to get the lat/long from each address.

First, let's set a constant for our base URL, and abstract out our directions into lists.
#+begin_src python
base_url = 'https://www.baltimorepolice.org/find-my-district'

vertical_directions = ['north', 'south']
horizontal_directions = ['east', 'west']
#+end_src

Let's start by setting our central station.
#+begin_src python
stations = ['central']
#+end_src

Now, let's add in each compass direction, appending an "ern" to the end of each one, i.e "east" becomes "eastern".
#+begin_src python
for direction in vertical_directions + horizontal_directions:
    stations.append(f'{direction}ern')
#+end_src

Next, we'll add the compound directions, which are formed by joining a vertical and horizontal direction, followed by "ern" like before.
#+begin_src python
for vertical in vertical_directions:
    for horizontal in horizontal_directions:
        stations.append(f'{vertical}{horizontal}ern')
#+end_src

Now that we have a list of all of our stations, let's make a dictionary mapping each station to its address.
First, let's write a function that will lookup the address of a single station.
#+begin_src python
address_pattern = re.compile(r'Address:Â (.+)')

def police_lookup_address(station: str) -> str:
    r = requests.get(f'{base_url}/{station}-district')
    soup = BeautifulSoup(r.text)
    combined_text = soup.get_text()
    search_result = address_pattern.search(combined_text)

    # return the first capture group
    return search_result.group(1)
#+end_src

Now, let's make a DataFrame for our stations.
#+begin_src python
stations_df = pd.DataFrame.from_dict({'station': stations})
#+end_src

Let's add a row for the address of each station.
#+begin_src python
stations_df['address'] = stations_df.apply(delay_fn(lambda row: police_lookup_address(row.station)), axis=1)
print(stations_df)
#+end_src

Next, let's use ~geopandas~ to convert each one of those addresses into a latitude and longitude.
#+begin_src python
stations_geocoded = gpd.tools.geocode(stations_df.address)
print(stations_geocoded)
#+end_src

We don't need the ~station~ column anymore, and the geocoded ~address~ is superior (more detailed) to the original, so we will replace the initial dataframe with the new one entirely.
#+begin_src python
stations_df = stations_geocoded
#+end_src

Finally, let's write our data to the file specified in the SPEC.
The data is tightly coupled to our scraping logic, so we write directly to ~dynamic~ rather than ~static~.
#+begin_src python
stations_df.to_csv(dynamic_filepath('police-stations'))
#+end_src
*** Population Density
For each district, we will have a coefficient representing population density.

First, let's get the [[https://data.baltimorecity.gov/datasets/bniajfi::total-population-community-statistical-area/explore?location=39.284832%2C-76.620524%2C12.65][Total Population]] dataset from Open Baltimore.
#+begin_src python
download_url = r'https://opendata.arcgis.com/api/v3/datasets/56d5b4e5480049e98315c2732aa48437_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1'

download_filepath = static_filepath('population-density')
download_cached(download_url, download_filepath)
#+end_src

Now, let's read it into a DataFrame.
#+begin_src python
populations_df = gpd.read_file(download_filepath)
print(populations_df.head())
#+end_src

We will use the average of the population value from 2010 and 2020.
#+begin_src python
populations_df = populations_df.assign(density=lambda df: (df['tpop10'] + df['tpop20']) / df['Shape__Area'])
#+end_src

Let's normalize the density values between -1 and 1, since the actual values themselves are less important than the values with relation to one another.
We are using a variant of min-max normalization that puts values between -1 and 1 rather than 0 and 1.
#+begin_src python
populations_df['density'] = normalize_min_max(populations_df['density'])
#+end_src

Let's now drop our unneeded ~tpop~ columns.
#+begin_src python
populations_df = populations_df.drop(['tpop10', 'tpop20'], axis=1)
#+end_src

Finally, let's export our dataframe to a CSV, specifying the *dynamic* filepath so that we won't have to redownload our data if we choose to change our data processing logic.
#+begin_src python
populations_df.to_csv(dynamic_filepath('population-density'))
#+end_src

*** Median Income
For Median Household Income, we will use the [[https://data.baltimorecity.gov/datasets/bniajfi::median-household-income/explore?layer=0&showTable=true][Median Household Income]] dataset from Open Baltimore.
#+begin_src python
download_url = r'https://opendata.arcgis.com/api/v3/datasets/8613366cfbc7447a9efd9123604c65c1_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1'

download_filepath = static_filepath('median-income')
download_cached(download_url, download_filepath)
#+end_src

Now, let's read it into a DataFrame.
#+begin_src python
median_income_df = gpd.read_file(download_filepath)
print(median_income_df.head())
#+end_src

It gives us median household income from 2010 until 2021.
For consistency, we will choose the middle year: 2016 as our standard.
#+begin_src python
median_income_df = median_income_df[['CSA2010', 'mhhi16', 'Shape__Area', 'Shape__Length', 'geometry']].rename(columns={'mhhi16': 'PopulationDensity'})
#+end_src

Let's now normalize our ~PopulationDensity~ column.
#+begin_src python
median_income_df['PopulationDensity'] = normalize_min_max(median_income_df['PopulationDensity'])
#+end_src

Finally, let's write it to a GeoJSON file.
#+begin_src python
median_income_df.to_file(dynamic_filepath('median-income'), driver='GeoJSON')
#+end_src
#+end_src

* TODO Data Processing
This is when we clean and normalize our data, preparing it for interpretation and analysis.
* TODO Exploratory Analysis & Data Visualization
This is where we will see what our data is telling us, so that we can make better judgements on what to look at for interpretation.
* TODO Interpretation/Conclusion
This is where we will draw conclusions from our data.
* File Config :noexport:
This is some Emacs configuration I have autoload when I open my notebook file.

Local Variables:
End:
