#+title: Baltimore Crime Analysis
#+property: header-args:python :session ./.jupyter_confile.json :kernel python3 :results output :noweb yes
#+property: CLEAN-EXPORT-FILENAME ./baltimore-crime-analysis_clean.ipynb.org

* TODO Introduction
** TODO Description of Project :ignore:
* General Setup
Let's import the core libraries that we will be using in our project.
#+begin_src python
import requests
import pandas as pd
import plotly.express as px
import geopandas as gpd

from bs4 import BeautifulSoup
import re
from time import sleep
#+end_src

* TODO Data Collection
We want all of our data to lie in the ~data/~ subdirectory.

First, let's make a SPEC matching each dataset/topic to file(s) that they will be saved in.
#+begin_src python
data_files = {
    'crime-data': 'crime_data.csv',
    'police-stations': 'police_stations.csv',
    'population-density': 'total_populations.csv',
    'median-income': 'median_incomes.geojson',
}
#+end_src

We only want to download the data once, so that future re-runs do not result in downloading it again.
To accomplish this, we will make a distinction between *static* and *dynamic* data.
* Static* data is data that we can directly from the internet without modification.
* Dynamic* data is data that we /compute/ from static data.
The benefit of this is that when we change our dynamic computation logic, we do not have to redownload our static data.

This function will, given the "tag" of the topic, yield a filepath for the static data.
For example, ~static_filepath('crime-data')~ would yield ~data/static/crime_data.csv~.
#+begin_src python
def static_filepath(tag) -> str:
    return f'data/static/{data_files[tag]}'
#+end_src

Similarly, ~dynamic_filepath~ will yield the filepath after post-processing.
#+begin_src python
def dynamic_filepath(tag) -> str:
    return f'data/dynamic/{data_files[tag]}'
#+end_src

Now, let's make our static and dynamic directories incase they don't exist.
#+begin_src python
import os
for d in ['dynamic', 'static']:
    try:
        os.mkdir(f'data/{d}')
    except FileExistsError:
        pass
#+end_src

** Helper Functions

*** Rate-limiting
Let's write a higher-order function that, given an Iterable, will return a Generator which will wait a certain amount of time between yielding elements.
#+begin_src python
from typing import Iterable, Generator

def delay_iterable(iterable: Iterable, delay_seconds=0.5) -> Generator:
    is_first_element = True
    for element in iterable:
        if not is_first_element:
            sleep(delay_seconds)
        else:
            is_first_element = False

        yield element
#+end_src

Let's write another function for simply delaying repeated function calls.
#+begin_src python
def delay_fn(f, delay_seconds=0.5):
    is_first_time = True

    def delayed_fn(*args, **kwargs):
        nonlocal f, is_first_time

        if not is_first_time:
            sleep(delay_seconds)
        else:
            is_first_time = False

        return f(*args, **kwargs)

    return delayed_fn
#+end_src
*** Caching Data
We don't want to refetch our data every time we run the notebook, so let's write some helper functions that allow us to cache our data after we collect it.
#+begin_src python
from os import path

def download_cached(url: str, filepath: str):
    # if the file already exists, does nothing.
    # otherwise, downloads to filepath
    if not path.exists(filepath):
        with open(filepath, 'w+') as f:
            f.write(requests.get(url).text)
#+end_src
*** Normalizing Data
This function will normalize a Series between -1 and 1.
This makes things easier for our machine learning model.
#+begin_src python
def normalize_min_max(series: 'Series') -> 'Series':
    return 2 * (series - series.min()) / (series.max() - series.min()) - 1
#+end_src

** One-time Data Collection
We only want to download the data once, so that future re-runs do not result in downloading it again.
*** Police Stations
There are 9 districts in Baltimore, corresponding to the 4 cardinal directions, 4 in-betweens and one central district.
To get the locations of the police stations in Baltimore, we will webscrape https://www.baltimorepolice.org/find-my-district, get the addresses of each station, and then use ~geopy~ to get the lat/long from each address.

First, let's set a constant for our base URL, and abstract out our directions into lists.
#+begin_src python
base_url = 'https://www.baltimorepolice.org/find-my-district'

vertical_directions = ['north', 'south']
horizontal_directions = ['east', 'west']
#+end_src

Let's start by setting our central station.
#+begin_src python
stations = ['central']
#+end_src

Now, let's add in each compass direction, appending an "ern" to the end of each one, i.e "east" becomes "eastern".
#+begin_src python
for direction in vertical_directions + horizontal_directions:
    stations.append(f'{direction}ern')
#+end_src

Next, we'll add the compound directions, which are formed by joining a vertical and horizontal direction, followed by "ern" like before.
#+begin_src python
for vertical in vertical_directions:
    for horizontal in horizontal_directions:
        stations.append(f'{vertical}{horizontal}ern')
#+end_src

Now that we have a list of all of our stations, let's make a dictionary mapping each station to its address.
First, let's write a function that will lookup the address of a single station.
#+begin_src python
address_pattern = re.compile(r'Address:Â (.+)')

def police_lookup_address(station: str) -> str:
    r = requests.get(f'{base_url}/{station}-district')
    soup = BeautifulSoup(r.text)
    combined_text = soup.get_text()
    search_result = address_pattern.search(combined_text)

    # return the first capture group
    return search_result.group(1)
#+end_src

Now, let's make a DataFrame for our stations.
#+begin_src python
stations_df = pd.DataFrame.from_dict({'station': stations})
#+end_src

Let's add a row for the address of each station.
#+begin_src python
stations_df['address'] = stations_df.apply(delay_fn(lambda row: police_lookup_address(row.station)), axis=1)
print(stations_df)
#+end_src

Next, let's use ~geopandas~ to convert each one of those addresses into a latitude and longitude.
#+begin_src python
stations_geocoded = gpd.tools.geocode(stations_df.address)
print(stations_geocoded)
#+end_src

We don't need the ~station~ column anymore, and the geocoded ~address~ is superior (more detailed) to the original, so we will replace the initial dataframe with the new one entirely.
#+begin_src python
stations_df = stations_geocoded
#+end_src

Finally, let's write our data to the file specified in the SPEC.
The data is tightly coupled to our scraping logic, so we write directly to ~dynamic~ rather than ~static~.
#+begin_src python
stations_df.to_csv(dynamic_filepath('police-stations'))
#+end_src
*** Population Density
For each district, we will have a coefficient representing population density.

First, let's get the [[https://data.baltimorecity.gov/datasets/bniajfi::total-population-community-statistical-area/explore?location=39.284832%2C-76.620524%2C12.65][Total Population]] dataset from Open Baltimore.
#+begin_src python
download_url = r'https://opendata.arcgis.com/api/v3/datasets/56d5b4e5480049e98315c2732aa48437_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1'

download_filepath = static_filepath('population-density')
download_cached(download_url, download_filepath)
#+end_src

Now, let's read it into a DataFrame.
#+begin_src python
populations_df = gpd.read_file(download_filepath)
print(populations_df.head())
#+end_src

We will use the average of the population value from 2010 and 2020.
#+begin_src python
populations_df = populations_df.assign(density=lambda df: (df['tpop10'] + df['tpop20']) / df['Shape__Area'])
#+end_src

Let's normalize the density values between -1 and 1, since the actual values themselves are less important than the values with relation to one another.
We are using a variant of min-max normalization that puts values between -1 and 1 rather than 0 and 1.
#+begin_src python
populations_df['density'] = normalize_min_max(populations_df['density'])
#+end_src

Let's now drop our unneeded ~tpop~ columns.
#+begin_src python
populations_df = populations_df.drop(['tpop10', 'tpop20'], axis=1)
#+end_src

Finally, let's export our dataframe to a CSV, specifying the *dynamic* filepath so that we won't have to redownload our data if we choose to change our data processing logic.
#+begin_src python
populations_df.to_csv(dynamic_filepath('population-density'))
#+end_src

#+begin_src python
#+end_src

* TODO Data Processing
This is when we clean and normalize our data, preparing it for interpretation and analysis.
* TODO Exploratory Analysis & Data Visualization
This is where we will see what our data is telling us, so that we can make better judgements on what to look at for interpretation.
* TODO Interpretation/Conclusion
This is where we will draw conclusions from our data.
* File Config :noexport:
This is some Emacs configuration I have autoload when I open my notebook file.
~org-babel-clean-autoexport-mode~ is a minor mode in my configuration which
automatically saves dirty notebooks to the file specified in
~CLEAN-EXPORT-FILENAME~, removing any ~:RESULTS:~ in the output.  This has the
effect of making the document easily version-controllable, since the variable
outputs of each code block do not mess with the ~diff~.

Local Variables:
eval: (org-babel-clean-autoexport-mode)
End:
